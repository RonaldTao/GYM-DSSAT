{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93dda018",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d47b8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdish as dd\n",
    "memory = dd.io.load('memory_tp1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734f28fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: rant3. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f1cfcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from IPython import display\n",
    "from collections import namedtuple, deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cpu\")\n",
    "# use cpu run\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8e0e3",
   "metadata": {},
   "source": [
    "### 2. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d74676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2array(state):\n",
    "    new_state = []\n",
    "    for key  in state.keys():\n",
    "        if key != 'sw':\n",
    "            new_state.append(state[key])\n",
    "        else:\n",
    "            new_state += list(state['sw'])        \n",
    "    state = np.asarray(new_state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2b4f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2array_partial(state):\n",
    "    new_state = []\n",
    "    num_observable_states = 0\n",
    "    for key  in state.keys():\n",
    "        if key != 'sw':\n",
    "            if key == 'cumsumfert':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'dap':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'dtt':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'istage':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'pltpop':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'rain':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'srad':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'tmax':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'tmin':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'vstage':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'xlai':\n",
    "                new_state.append(state[key])\n",
    "        else:\n",
    "            new_state += list(state['sw'])\n",
    "    state = np.asarray(new_state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8650bd",
   "metadata": {},
   "source": [
    "### 3. Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe07f4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: {'cleach': 0.0, 'cnox': 0.0, 'cumsumfert': 0.0, 'dap': 0, 'dtt': 0.0, 'es': 0.0, 'grnwt': 0.0, 'istage': 7, 'nstres': 0.0, 'pcngrn': 0.0, 'pltpop': 7.199999809265137, 'rain': 0.0, 'rtdep': 0.0, 'runoff': 0.0, 'srad': 13.300000190734863, 'sw': array([0.086     , 0.086     , 0.086     , 0.086     , 0.086     ,\n",
      "       0.076     , 0.076     , 0.13      , 0.25799999]), 'swfac': 0.0, 'tleachd': 0.0, 'tmax': 22.200000762939453, 'tmin': 3.299999952316284, 'tnoxd': 0.0, 'topwt': 0.0, 'trnu': 0.0, 'vstage': 0.0, 'wtdep': 0.0, 'wtnup': 0.0, 'xlai': 0.0}\n",
      "27 9\n",
      "\n",
      "Ram information received from DASSAT will has 20 dimensions.\n",
      "There are 25 possible actions at each step.\n",
      "Discrete? False\n"
     ]
    }
   ],
   "source": [
    "env_args = {\n",
    "    'run_dssat_location': '/opt/dssat_pdi/run_dssat',  # assuming (modified) DSSAT has been installed in /opt/dssat_pdi\n",
    "    'log_saving_path': './logs/dssat-pdi.log',  # if you want to save DSSAT outputs for inspection\n",
    "    # 'mode': 'irrigation',  # you can choose one of those 3 modes\n",
    "    # 'mode': 'fertilization',\n",
    "    'mode': 'all',\n",
    "    'seed': 123456,\n",
    "    'random_weather': False,  # if you want stochastic weather\n",
    "}\n",
    "env = gym.make('gym_dssat_pdi:GymDssatPdi-v0', **env_args)\n",
    "print('Observation:',env.observation,)\n",
    "print(len(env.observation),len(env.observation['sw']))\n",
    "ram_dimensions = 20\n",
    "nb_actions = 25\n",
    "print('\\nRam information received from DASSAT will has %d dimensions.' % ram_dimensions)\n",
    "print('There are %d possible actions at each step.' % nb_actions)\n",
    "print('Discrete?',type(gym.spaces)== gym.spaces.Discrete)\n",
    "# observation has 27 elements, 9 values in soil water\n",
    "# state size = 27+8 dimension\n",
    "# how to defind nb_action? why is 200?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4283d29",
   "metadata": {},
   "source": [
    "### 4. Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58521ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_QNetwork(nn.Module):\n",
    "    \"\"\"Agent (Policy) Model.\"\"\"\n",
    "    # given a state of 35 dim, Qnetwork will return 200 values for each possible action  \n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units=128*2,fc2_units=128*2,fc3_units=128*2):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            why is it 256? randomly?\n",
    "        \"\"\"\n",
    "        super(Full_QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units, action_size)\n",
    "        # set a nn with 1 layer\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        y = F.relu(self.fc2(x))\n",
    "        z = F.relu(self.fc3(y))\n",
    "        #Applies the rectified linear unit function element-wise. max(0,x)\n",
    "        return self.fc4(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c665be2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = Full_QNetwork(35, 25)\n",
    "trained_model.load_state_dict(torch.load('/home/rant3/focal/IAAI/Trained_Policies/FL_Economic_Full/new1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61449943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, trained_model):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # Q-Network\n",
    "        self.qnetwork = trained_model\n",
    "        #self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
    "\n",
    "        # Replay memory\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork(state)\n",
    "        self.qnetwork.train()\n",
    "\n",
    "#         Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "#         Epsilon-greedy action selection\n",
    "#         if random.random() > eps:\n",
    "#             return np.argmax(action_values.cpu().data.numpy())\n",
    "#         else:\n",
    "#             return random.choice(np.arange(self.action_size))\n",
    "# #         return action_values.cpu().data.nump\n",
    "    def get(self,state):\n",
    "        self.qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            output= self.qnetwork(state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33035fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Partial_QNetwork(nn.Module):\n",
    "    \"\"\"Agent (Policy) Model.\"\"\"\n",
    "    # given a state of 35 dim, Qnetwork will return 200 values for each possible action  \n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units=128*2,fc2_units=128*2,fc3_units=128*2):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            why is it 256? randomly?\n",
    "        \"\"\"\n",
    "        super(Partial_QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units, action_size)\n",
    "        # set a nn with 1 layer\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        y = F.relu(self.fc2(x))\n",
    "        z = F.relu(self.fc3(y))\n",
    "        #Applies the rectified linear unit function element-wise. max(0,x)\n",
    "        return 4*torch.sigmoid(self.fc4(z))*torch.tensor([[float(40),float(6)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38b50707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, LR):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # Q-Network\n",
    "        self.qnetwork = Partial_QNetwork(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork.parameters(), lr=LR)\n",
    "        self.targetwork = trained_model\n",
    "    \n",
    "    \n",
    "    def learn_1(self,sp,input1):\n",
    "        sp=torch.from_numpy(sp).float().unsqueeze(0).to(device)\n",
    "        input2=self.qnetwork(sp)\n",
    "        input2=input2.type(torch.float64)\n",
    "        print('input1 is', input1)\n",
    "        print('input2 is', input2)\n",
    "        #if round(input2.item())!= 0:\n",
    "        #    print('input1 is', input1.item())\n",
    "        #    print('input2 is', round(input2.item()))\n",
    "        #    print(episode)\n",
    "        print('')\n",
    "        loss = F.mse_loss(input1, input2)\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # after this, the parameter of local network will change based on gradient descent\n",
    "        for param in self.qnetwork.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        # stabilize traning to keep grad between (-1,1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def learn_2(self,memory,batch_size):\n",
    "        sample=random.sample(memory,batch_size)\n",
    "        input1=sample[0][2]\n",
    "        #print(input1)\n",
    "        state=torch.from_numpy(sample[0][1]).float().unsqueeze(0)\n",
    "        input2=self.qnetwork(state)\n",
    "        input2=input2.type(torch.float64)\n",
    "        for i in range(1,batch_size):\n",
    "            input1=torch.vstack((input1,sample[i][2]))\n",
    "            state=torch.from_numpy(sample[i][1]).float().unsqueeze(0).to(device)\n",
    "            v=self.qnetwork(state)\n",
    "            v=v.type(torch.float64)\n",
    "            input2=torch.vstack((input2,v))\n",
    "        loss = F.mse_loss(input1, input2)\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # after this, the parameter of local network will change based on gradient descent\n",
    "        for param in self.qnetwork.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        # stabilize traning to keep grad between (-1,1)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def save(self,name):\n",
    "        torch.save(self.qnetwork.state_dict(),'/home/rant3/focal/model'+name+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96e642d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_agent = Agent(state_size=ram_dimensions, action_size=2, LR = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d0f2430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_partial(episodes,memory,batch_size,exp=1):\n",
    "    wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"new super partial\", \n",
    "      # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"experiment_{exp}\",)\n",
    "    loss_window=deque(maxlen=1000)\n",
    "    loss_list=[]\n",
    "    for i in range(1,episodes+1):\n",
    "        #sp = random.choice(memory)\n",
    "        #print(sp)\n",
    "        #par_state=sp[1]\n",
    "        #full_sp = torch.from_numpy(full_sp).float().unsqueeze(0).to(device)\n",
    "        #input1=sp[2]\n",
    "        #input1=torch.tensor([[float(0)]])\n",
    "        partial_agent.learn_2(memory,batch_size)\n",
    "        #print('loss is',loss)\n",
    "        #loss_list.append(loss.item())\n",
    "        #loss_window.append(loss.item())\n",
    "        #if i % 1000 == 0:\n",
    "            #print('Average loss is', np.mean(loss_window))\n",
    "        if i>14000:\n",
    "            if i%200==0:\n",
    "                partial_agent.save(str(i))\n",
    "            #partial_agent.save(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "306f1ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rant3/focal/wandb/run-20221011_192406-34khcnax</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rant3/new%20super%20partial/runs/34khcnax\" target=\"_blank\">experiment_1</a></strong> to <a href=\"https://wandb.ai/rant3/new%20super%20partial\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: ERROR Error while calling W&B API: internal database error (<Response [500]>)\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "super_partial(15000,memory,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12fa4035",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (899350283.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [15]\u001b[0;36m\u001b[0m\n\u001b[0;31m    d=torch.tensor(np.vstack((a,b)))n\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([[float(1),float(2)]])\n",
    "b=torch.tensor([[float(3),float(4)]])\n",
    "d=torch.tensor(np.vstack((a,b)))n\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e604881",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([float(5),float(2)])\n",
    "b=torch.tensor([float(8),float(4)])\n",
    "f=torch.tensor(np.vstack((a,b)))\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675a9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=F.mse_loss(d,f)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e479779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[-1., -1.],\n",
      "        [-1., -1.],\n",
      "        [-1., -1.],\n",
      "        [-1., -1.],\n",
      "        [-1., -1.],\n",
      "        [-1., -1.],\n",
      "        [-1., -1.],\n",
      "        [-1., -1.],\n",
      "        [-1., -1.],\n",
      "        [-1., -1.],\n",
      "        [-1., -1.]])\n",
      "tensor(4.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45847/3456370731.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  f=torch.tensor(torch.vstack((a,a)))\n",
      "/tmp/ipykernel_45847/3456370731.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  f=torch.tensor(torch.vstack((f,a)))\n",
      "/tmp/ipykernel_45847/3456370731.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  g=torch.tensor(torch.vstack((b,b)))\n",
      "/tmp/ipykernel_45847/3456370731.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  g=torch.tensor(torch.vstack((g,b)))\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([float(1),float(1)])\n",
    "f=torch.tensor(torch.vstack((a,a)))\n",
    "for i in range(9):\n",
    "    f=torch.tensor(torch.vstack((f,a)))\n",
    "b=torch.tensor([float(-1),float(-1)])\n",
    "g=torch.tensor(torch.vstack((b,b)))\n",
    "for i in range(9):\n",
    "    g=torch.tensor(torch.vstack((g,b)))\n",
    "print(f)\n",
    "print(g)\n",
    "print(F.mse_loss(f,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5919a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
