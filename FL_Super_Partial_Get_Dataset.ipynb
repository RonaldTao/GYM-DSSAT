{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd65f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from IPython import display\n",
    "from collections import namedtuple, deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cpu\")\n",
    "# use cpu run\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb065231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2array(state):\n",
    "    new_state = []\n",
    "    for key  in state.keys():\n",
    "        if key != 'sw':\n",
    "            new_state.append(state[key])\n",
    "        else:\n",
    "            new_state += list(state['sw'])        \n",
    "    state = np.asarray(new_state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b934b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2array_partial(state):\n",
    "    new_state = []\n",
    "    num_observable_states = 0\n",
    "    for key  in state.keys():\n",
    "        if key != 'sw':\n",
    "            if key == 'cumsumfert':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'dap':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'dtt':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'istage':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'pltpop':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'rain':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'srad':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'tmax':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'tmin':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'vstage':\n",
    "                new_state.append(state[key])\n",
    "            if key == 'xlai':\n",
    "                new_state.append(state[key])\n",
    "        else:\n",
    "            new_state += list(state['sw'])\n",
    "    state = np.asarray(new_state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2efb1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: {'cleach': 0.0, 'cnox': 0.0, 'cumsumfert': 0.0, 'dap': 0, 'dtt': 0.0, 'es': 0.0, 'grnwt': 0.0, 'istage': 7, 'nstres': 0.0, 'pcngrn': 0.0, 'pltpop': 7.199999809265137, 'rain': 0.0, 'rtdep': 0.0, 'runoff': 0.0, 'srad': 13.300000190734863, 'sw': array([0.086     , 0.086     , 0.086     , 0.086     , 0.086     ,\n",
      "       0.076     , 0.076     , 0.13      , 0.25799999]), 'swfac': 0.0, 'tleachd': 0.0, 'tmax': 22.200000762939453, 'tmin': 3.299999952316284, 'tnoxd': 0.0, 'topwt': 0.0, 'trnu': 0.0, 'vstage': 0.0, 'wtdep': 0.0, 'wtnup': 0.0, 'xlai': 0.0}\n",
      "27 9\n",
      "\n",
      "Ram information received from DASSAT will has 20 dimensions.\n",
      "There are 25 possible actions at each step.\n",
      "Discrete? False\n"
     ]
    }
   ],
   "source": [
    "env_args = {\n",
    "    'run_dssat_location': '/opt/dssat_pdi/run_dssat',  # assuming (modified) DSSAT has been installed in /opt/dssat_pdi\n",
    "    'log_saving_path': './logs/dssat-pdi.log',  # if you want to save DSSAT outputs for inspection\n",
    "    # 'mode': 'irrigation',  # you can choose one of those 3 modes\n",
    "    # 'mode': 'fertilization',\n",
    "    'mode': 'all',\n",
    "    'seed': 123456,\n",
    "    'random_weather': False,  # if you want stochastic weather\n",
    "}\n",
    "env = gym.make('gym_dssat_pdi:GymDssatPdi-v0', **env_args)\n",
    "print('Observation:',env.observation,)\n",
    "print(len(env.observation),len(env.observation['sw']))\n",
    "ram_dimensions = 20\n",
    "nb_actions = 25\n",
    "print('\\nRam information received from DASSAT will has %d dimensions.' % ram_dimensions)\n",
    "print('There are %d possible actions at each step.' % nb_actions)\n",
    "print('Discrete?',type(gym.spaces)== gym.spaces.Discrete)\n",
    "# observation has 27 elements, 9 values in soil water\n",
    "# state size = 27+8 dimension\n",
    "# how to defind nb_action? why is 200?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a158b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_QNetwork(nn.Module):\n",
    "    \"\"\"Agent (Policy) Model.\"\"\"\n",
    "    # given a state of 35 dim, Qnetwork will return 200 values for each possible action  \n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units=128*2,fc2_units=128*2,fc3_units=128*2):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            why is it 256? randomly?\n",
    "        \"\"\"\n",
    "        super(Full_QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units, action_size)\n",
    "        # set a nn with 1 layer\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        y = F.relu(self.fc2(x))\n",
    "        z = F.relu(self.fc3(y))\n",
    "        #Applies the rectified linear unit function element-wise. max(0,x)\n",
    "        return self.fc4(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae4ec5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = Full_QNetwork(35, 25)\n",
    "trained_model.load_state_dict(torch.load('/home/rant3/focal/IAAI/Trained_Policies/FL_Economic_Full/new1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ec072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, trained_model):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # Q-Network\n",
    "        self.qnetwork = trained_model\n",
    "        #self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
    "\n",
    "        # Replay memory\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork(state)\n",
    "        self.qnetwork.train()\n",
    "\n",
    "#         Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "#         Epsilon-greedy action selection\n",
    "#         if random.random() > eps:\n",
    "#             return np.argmax(action_values.cpu().data.numpy())\n",
    "#         else:\n",
    "#             return random.choice(np.arange(self.action_size))\n",
    "# #         return action_values.cpu().data.nump\n",
    "    def get(self,state):\n",
    "        self.qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            output= self.qnetwork(state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "138bc9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_agent= Full_Agent(35,25,trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d831e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdish as dd\n",
    "def get_memory(episodes):\n",
    "    memory=[]\n",
    "    for i in range(episodes):\n",
    "        env.reset()\n",
    "        state=env.observation\n",
    "        full_state = dict2array(state)\n",
    "        par_state = dict2array_partial(state)\n",
    "        for t in range(500):\n",
    "            #action1 = full_agent.act(full_state,0.1)\n",
    "            action2 = full_agent.act(full_state,0)\n",
    "            a = np.array([[float((action2%5)*40),float(int(action2/5)*6)]])\n",
    "            #if action!= 0:\n",
    "            #    memory.append(state)\n",
    "            #action1 = full_agent.act(state,0.1)\n",
    "            action = {\n",
    "                    'anfer': (action2%5)*40,  # if mode == fertilization or mode == all ; nitrogen to fertilize in kg/ha\n",
    "                    'amir': int(action2/5)*6,  # if mode == irrigation or mode == all ; water to irrigate in L/ha\n",
    "            }\n",
    "            if action2 == 5:\n",
    "                memory.append((full_state,par_state,torch.tensor(a)))\n",
    "                memory.append((full_state,par_state,torch.tensor(a)))\n",
    "                memory.append((full_state,par_state,torch.tensor(a)))\n",
    "                memory.append((full_state,par_state,torch.tensor(a)))\n",
    "                memory.append((full_state,par_state,torch.tensor(a)))\n",
    "            elif action2 == 6:\n",
    "                for j in range(20):\n",
    "                    memory.append((full_state,par_state,torch.tensor(a)))\n",
    "            elif action2 == 1:\n",
    "                for j in range(15):\n",
    "                    memory.append((full_state,par_state,torch.tensor(a)))\n",
    "            else:\n",
    "                memory.append((full_state,par_state,torch.tensor(a)))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                #print(i)\n",
    "                break\n",
    "            state = next_state\n",
    "            full_state = dict2array(state)\n",
    "            par_state = dict2array_partial(state)\n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c57e8379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory=get_memory(1)\n",
    "dd.io.save('memory_tp1.h5', memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37cba370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309\n"
     ]
    }
   ],
   "source": [
    "memory = dd.io.load('memory_tp1.h5')\n",
    "print(len(memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dffce740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.]], dtype=torch.float64)\n",
      "tensor([[0., 6.]], dtype=torch.float64)\n",
      "tensor([[40.,  6.]], dtype=torch.float64)\n",
      "tensor([[0., 6.]], dtype=torch.float64)\n",
      "tensor([[0., 6.]], dtype=torch.float64)\n",
      "tensor([[0., 6.]], dtype=torch.float64)\n",
      "tensor([[40.,  0.]], dtype=torch.float64)\n",
      "tensor([[40.,  6.]], dtype=torch.float64)\n",
      "tensor([[0., 0.]], dtype=torch.float64)\n",
      "tensor([[0., 6.]], dtype=torch.float64)\n",
      "tensor([[0., 0.]], dtype=torch.float64)\n",
      "tensor([[0., 0.]], dtype=torch.float64)\n",
      "tensor([[40.,  6.]], dtype=torch.float64)\n",
      "tensor([[0., 6.]], dtype=torch.float64)\n",
      "tensor([[40.,  6.]], dtype=torch.float64)\n",
      "tensor([[0., 6.]], dtype=torch.float64)\n",
      "tensor([[0., 0.]], dtype=torch.float64)\n",
      "tensor([[0., 6.]], dtype=torch.float64)\n",
      "tensor([[0., 0.]], dtype=torch.float64)\n",
      "tensor([[40.,  0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    sp=random.choice(memory)\n",
    "    print(sp[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00cd87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
