{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93dda018",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f1cfcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from IPython import display\n",
    "from collections import namedtuple, deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cpu\")\n",
    "# use cpu run\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8e0e3",
   "metadata": {},
   "source": [
    "### 2. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d74676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2array(state):\n",
    "    new_state = []\n",
    "    for key  in state.keys():\n",
    "        if key != 'sw':\n",
    "            new_state.append(state[key])\n",
    "        else:\n",
    "            new_state += list(state['sw'])        \n",
    "    state = np.asarray(new_state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98903544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2arrayN(state):\n",
    "    new_state = []\n",
    "    for key  in state.keys():\n",
    "        if key != 'sw':\n",
    "            new_state.append(state[key])\n",
    "        else:\n",
    "            new_state += list(state['sw'])        \n",
    "    state = np.asarray(new_state)\n",
    "    state = state-np.array([0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-60,-60,0,0,0,0,0,0,0])\n",
    "    weight = np.array([200,5,250*160,366,100,50,50000,9,1,1,100,500,300,\n",
    "                       100,200,1,1,1,1,1,1,1,1,1,1,1,1,100,120,120,1,5000,10,30,1000,500,10])\n",
    "    weight2 = np.array([0.01,1,400,1,1,1,500,0.1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,50,1,0.1,1,1,1])\n",
    "    state = state/weight2               \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8650bd",
   "metadata": {},
   "source": [
    "### 3. Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe07f4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: {'cleach': 0.0, 'cnox': 0.0, 'cumsumfert': 0.0, 'dap': 0, 'dtt': 0.0, 'es': 0.0, 'grnwt': 0.0, 'istage': 7, 'nstres': 0.0, 'pcngrn': 0.0, 'pltpop': 7.599999904632568, 'rain': 0.0, 'rtdep': 0.0, 'runoff': 0.0, 'srad': 26.5, 'sw': array([0.189     , 0.189     , 0.189     , 0.222     , 0.228     ,\n",
      "       0.27700001, 0.31      , 0.31799999, 0.29800001, 0.28      ,\n",
      "       0.28      ]), 'swfac': 0.0, 'tleachd': 0.0, 'tmax': 16.100000381469727, 'tmin': 5.400000095367432, 'tnoxd': 0.0, 'topwt': 0.0, 'trnu': 0.0, 'vstage': 0.0, 'wtdep': 0.0, 'wtnup': 0.0, 'xlai': 0.0}\n",
      "27 11\n",
      "\n",
      "Ram information received from DASSAT will has 37 dimensions.\n",
      "There are 5 possible actions at each step.\n",
      "Discrete? False\n",
      "37\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.         60.          0.          0.          7.5999999   0.\n",
      "  0.          0.         26.5         0.189       0.189       0.189\n",
      "  0.222       0.228       0.27700001  0.31        0.31799999  0.29800001\n",
      "  0.28        0.28        0.          0.         76.10000038 65.4000001\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "env_args = {\n",
    "    'run_dssat_location': '/opt/dssat_pdi/run_dssat',  # assuming (modified) DSSAT has been installed in /opt/dssat_pdi\n",
    "    'log_saving_path': './logs/dssat-pdi.log',  # if you want to save DSSAT outputs for inspection\n",
    "    # 'mode': 'irrigation',  # you can choose one of those 3 modes\n",
    "    # 'mode': 'fertilization',\n",
    "    'mode': 'all',\n",
    "    'seed': 123456,\n",
    "    'random_weather': False,  # if you want stochastic weather\n",
    "    'fileX_template_path':'./IUAF9901.MZX',\n",
    "    #'experiment_number':1\n",
    "}\n",
    "env = gym.make('gym_dssat_pdi:GymDssatPdi-v0', **env_args)\n",
    "print('Observation:',env.observation,)\n",
    "print(len(env.observation),len(env.observation['sw']))\n",
    "ram_dimensions = len(env.observation)+len(env.observation['sw'])-1\n",
    "nb_actions = 5\n",
    "print('\\nRam information received from DASSAT will has %d dimensions.' % ram_dimensions)\n",
    "print('There are %d possible actions at each step.' % nb_actions)\n",
    "print('Discrete?',type(gym.spaces)== gym.spaces.Discrete)\n",
    "print(ram_dimensions)\n",
    "state = env.reset() # reset the environment and get current state\n",
    "state = dict2arrayN(state)\n",
    "print(state)\n",
    "# observation has 27 elements, 9 values in soil water\n",
    "# state size = 27+8 dimension\n",
    "# how to defind nb_action? why is 200?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4283d29",
   "metadata": {},
   "source": [
    "### 4. Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6021276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Agent (Policy) Model.\"\"\"\n",
    "    # given a state of 35 dim, Qnetwork will return 200 values for each possible action  \n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            why is it 256? randomly?\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, action_size)\n",
    "        # set a nn with 1 layer\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        #Applies the rectified linear unit function element-wise. max(0,x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705878c",
   "metadata": {},
   "source": [
    "### 5. Define the Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a8520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            how many samples stored in buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        # store [s,a,r,s',done] for each sample in buffer\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "#         action = dict2array(action)\n",
    "#         print('transfored actions:',action)\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "#         for e in experiences:\n",
    "# #             print(e)\n",
    "# #             print(e.state.shape, e.next_state.shape)\n",
    "#             break\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        # vstack is add rows together as a single matrix\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        #get all the states, actions, rewards, next_states, dones for all elements in this sample batch, each as a single matrix\n",
    "        #device here is cpu?\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72686c19",
   "metadata": {},
   "source": [
    "### 6. Define the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef9713bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 60                # for update of target network parameters\n",
    "LR = 1e-5               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea8e9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        # add current (s,r,a,s') and sample a batch and learn if update_every\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step += 1\n",
    "        if self.t_step%UPDATE_EVERY == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "#         Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        # return the action index with the largest value\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "            #return one action randomly with possibility eps\n",
    "\n",
    "#         Epsilon-greedy action selection\n",
    "#         if random.random() > eps:\n",
    "#             return np.argmax(action_values.cpu().data.numpy())\n",
    "#         else:\n",
    "#             return random.choice(np.arange(self.action_size))\n",
    "# #         return action_values.cpu().data.numpy()\n",
    "\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # detach for diemnsion correctness\n",
    "        # GET largest value of self.qnetwork_target(next_states)\n",
    "#         print(Q_targets_next)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # 0 if done\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "#         print('out',self.qnetwork_local(states).shape)\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        #Why need to gather? actions are indexs?\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # after this, the parameter of local network will change based on gradient descent\n",
    "        for param in self.qnetwork_local.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        # stabilize traning to keep grad between (-1,1)\n",
    "        self.optimizer.step()\n",
    "        #固定用法, change learning rate\n",
    "\n",
    "        #Update target network weights every TAU learning steps (so every TAU*UPDATE_EVERY t_step)\n",
    "        if self.t_step%(TAU*UPDATE_EVERY)==0:\n",
    "            self.update_target_net(self.qnetwork_local, self.qnetwork_target)                     \n",
    "\n",
    "    def update_target_net(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "        \"\"\"\n",
    "        target_model.load_state_dict(local_model.state_dict())\n",
    "    def save(self):\n",
    "        torch.save(self.qnetwork_local.state_dict(),'/home/rant3/focal/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c37fd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_costant_k1(state, action, next_state, done, k1, k2, action_amount):\n",
    "    #if done, return Yield (topwt) - k*cost\n",
    "    penalty = 0\n",
    "    if action_amount > 250:\n",
    "        if action!=0:\n",
    "            penalty = (action_amount - 250)\n",
    "    if done:\n",
    "        reward = state[31] - k2*action - k1*penalty\n",
    "        return reward\n",
    "    #if done, return Yield (topwt) - k*cost\n",
    "    else:\n",
    "        reward = -k2*(action + state[27]) - k1*penalty\n",
    "        return reward\n",
    "    # otherwise, return -k*(action+leaching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0889c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nget_reward_costant_k1(state, action, next_state, done, k1, k2, action_amount):\n",
    "    #if done, return Yield (topwt) - k*cost\n",
    "    penalty = 0\n",
    "    if action_amount > 250:\n",
    "        if action!=0:\n",
    "            penalty = (action_amount - 250)\n",
    "    if done:\n",
    "        reward = state[31]*50 - k2*action - k1*penalty\n",
    "        return reward\n",
    "    #if done, return Yield (topwt) - k*cost\n",
    "    else:\n",
    "        reward = -k2*(action + state[27]*1) - k1*penalty\n",
    "        return reward\n",
    "    # otherwise, return -k*(action+leaching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b4baf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r(state, action, done, action_amount):\n",
    "    #if done, return Yield (topwt) - k*cost\n",
    "    penalty = 0\n",
    "    #k1=10**(-3)*(i_episode*2)\n",
    "    # base cost is current input action\n",
    "    if action_amount > 250:\n",
    "        if action!=0:\n",
    "            penalty = (action_amount - 250)\n",
    "    if done:\n",
    "        reward = 0.158*state[6] - 0.88*action - 1*penalty\n",
    "        return reward\n",
    "    #if done, return Yield (topwt) - k*cost\n",
    "    else:\n",
    "        reward = -0.88*action-1*penalty\n",
    "        return reward\n",
    "    # otherwise, return -k*(action+leaching)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171c08a",
   "metadata": {},
   "source": [
    "### DQN for nitrogen management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8bd30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=ram_dimensions, action_size=nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28016e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=2000, max_t=500, eps_start=1.0, eps_end=0, eps_decay=0.996, solved=5000, optimize = True):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    start = time.time()                # Start time\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    list_eps = []\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    action_amount=0\n",
    "    action_amount_list = []\n",
    "    action_amount_window = deque(maxlen=100)\n",
    "    leaching_sum_list = []\n",
    "    leaching_sum_window = deque(maxlen=100)\n",
    "    Yield_list=[]\n",
    "    Yield_window=deque(maxlen=100)\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # Reset env and score at the beginning of episode\n",
    "        state = env.reset() # reset the environment and get current state\n",
    "        state = dict2arrayN(state)\n",
    "        #print(state)\n",
    "        score = 0 # initialize the score\n",
    "        action_amount = 0\n",
    "        leaching_sum = state[27]\n",
    "        for t in range(max_t):\n",
    "            action1 = agent.act(state, eps) if optimize else 2\n",
    "            action_amount = action_amount + action1*40\n",
    "            action = {\n",
    "                    'anfer': action1*40,  # if mode == fertilization or mode == all ; nitrogen to fertilize in kg/ha\n",
    "                    'amir': 0,  # if mode == irrigation or mode == all ; water to irrigate in L/ha\n",
    "            }\n",
    "            next_state, reward, done, _ = env.step(action)      # send the action to the environment\n",
    "            #reward = reward-\n",
    "            # add changes to reward to penalize multiple actions\n",
    "            # this step is different from agent step\n",
    "            if done:\n",
    "                #print(state)\n",
    "                next_state = np.ones(37)\n",
    "                reward = Nget_reward_costant_k1(state, action['anfer'], next_state, done, 1, 0.1, action_amount)\n",
    "                agent.step(state, action1, reward, next_state, done)\n",
    "                score += reward\n",
    "                leaching_sum_list.append(leaching_sum)\n",
    "                #print('Episode is ', i_episode, 'Yield is',state[29])\n",
    "                action_amount_window.append(action_amount)\n",
    "                leaching_sum_window.append(leaching_sum)\n",
    "                Yield_list.append(state[31]*500)\n",
    "                Yield_window.append(state[31]*500)\n",
    "                break\n",
    "            next_state = dict2arrayN(next_state)\n",
    "            reward = Nget_reward_costant_k1(state, action['anfer'], next_state, done, 1, 0.1, action_amount)\n",
    "            agent.step(state, action1, reward, next_state, done)\n",
    "            state = next_state\n",
    "            leaching_sum = leaching_sum + state[27]\n",
    "            score += reward\n",
    "            #append\n",
    "\n",
    "\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        \n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        list_eps.append(eps)\n",
    "        agent.save()\n",
    "        #if i_episode > 1400:\n",
    "        #    eps = 0\n",
    "        if i_episode % 40 == 0:\n",
    "            print('\\rEpisode {}/{} \\tAverage Score: {:.2f}'.format(i_episode, n_episodes, np.mean(scores_window)))\n",
    "            print('Epsilon: {}'.format(eps))\n",
    "            print('Average nitrogen amount is', np.mean(action_amount_window))\n",
    "            print('Average leaching sum is', np.mean(leaching_sum_window))\n",
    "            print('Average Yield is', np.mean(Yield_window))\n",
    "            \n",
    "        if np.mean(scores_window)>solved:\n",
    "            print('Game Solved after {} episodes'.format(i_episode))\n",
    "            break\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print(\"Time Elapse: {:.2f} seconds\".format(time_elapsed))\n",
    "    \n",
    "    return scores, list_eps, action_amount_list, leaching_sum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94815aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40/3000 \tAverage Score: -967929.44\n",
      "Epsilon: 0.8518704174613255\n",
      "Average nitrogen amount is 14059.0\n",
      "Average leaching sum is 0.38397804215577197\n",
      "Average Yield is 21694.948669433594\n",
      "Episode 80/3000 \tAverage Score: -835277.79\n",
      "Epsilon: 0.725683208145733\n",
      "Average nitrogen amount is 13000.0\n",
      "Average leaching sum is 0.36718874653828093\n",
      "Average Yield is 21694.952270507812\n",
      "Episode 120/3000 \tAverage Score: -654624.96\n",
      "Epsilon: 0.6181880574677795\n",
      "Average nitrogen amount is 11533.2\n",
      "Average leaching sum is 0.34011999368694373\n",
      "Average Yield is 21694.95703125\n",
      "Episode 160/3000 \tAverage Score: -469915.58\n",
      "Epsilon: 0.5266161185846833\n",
      "Average nitrogen amount is 9796.4\n",
      "Average leaching sum is 0.31088772421729516\n",
      "Average Yield is 21694.959057617187\n",
      "Episode 200/3000 \tAverage Score: -344495.33\n",
      "Epsilon: 0.4486086927805969\n",
      "Average nitrogen amount is 8426.0\n",
      "Average leaching sum is 0.2851562190244789\n",
      "Average Yield is 21694.95895996094\n",
      "Episode 240/3000 \tAverage Score: -247128.17\n",
      "Epsilon: 0.3821564743957867\n",
      "Average nitrogen amount is 7172.4\n",
      "Average leaching sum is 0.262907890603214\n",
      "Average Yield is 21694.953955078126\n",
      "Episode 280/3000 \tAverage Score: -179277.37\n",
      "Epsilon: 0.3255477953790873\n",
      "Average nitrogen amount is 6112.4\n",
      "Average leaching sum is 0.2449056443388457\n",
      "Average Yield is 21694.95280761719\n",
      "Episode 320/3000 \tAverage Score: -124263.21\n",
      "Epsilon: 0.27732453635319726\n",
      "Average nitrogen amount is 5128.8\n",
      "Average leaching sum is 0.2282898215393419\n",
      "Average Yield is 21694.958129882812\n",
      "Episode 360/3000 \tAverage Score: -87533.47\n",
      "Epsilon: 0.2362445685554666\n",
      "Average nitrogen amount is 4345.2\n",
      "Average leaching sum is 0.21321064474087906\n",
      "Average Yield is 21695.75380859375\n",
      "Episode 400/3000 \tAverage Score: -64483.24\n",
      "Epsilon: 0.20124975923831606\n",
      "Average nitrogen amount is 3775.2\n",
      "Average leaching sum is 0.1993208044770057\n",
      "Average Yield is 21696.96474609375\n",
      "Episode 440/3000 \tAverage Score: -46327.54\n",
      "Epsilon: 0.1714387164163356\n",
      "Average nitrogen amount is 3224.0\n",
      "Average leaching sum is 0.18540242267799328\n",
      "Average Yield is 21698.739697265624\n",
      "Episode 480/3000 \tAverage Score: -32885.23\n",
      "Epsilon: 0.14604357092261758\n",
      "Average nitrogen amount is 2769.2\n",
      "Average leaching sum is 0.1733228153179283\n",
      "Average Yield is 21700.055786132812\n",
      "Episode 520/3000 \tAverage Score: -23831.88\n",
      "Epsilon: 0.12441019772939296\n",
      "Average nitrogen amount is 2397.2\n",
      "Average leaching sum is 0.1619392018954153\n",
      "Average Yield is 21684.596520996092\n",
      "Episode 560/3000 \tAverage Score: -16165.92\n",
      "Epsilon: 0.10598136707618404\n",
      "Average nitrogen amount is 2044.8\n",
      "Average leaching sum is 0.15425578488995087\n",
      "Average Yield is 21673.745227050782\n",
      "Episode 600/3000 \tAverage Score: -11456.50\n",
      "Epsilon: 0.09028239141431091\n",
      "Average nitrogen amount is 1778.8\n",
      "Average leaching sum is 0.15002512635689924\n",
      "Average Yield is 21679.966088867186\n",
      "Episode 640/3000 \tAverage Score: -7115.35\n",
      "Epsilon: 0.07690889846351584\n",
      "Average nitrogen amount is 1492.4\n",
      "Average leaching sum is 0.14187148522612403\n",
      "Average Yield is 21640.03483886719\n",
      "Episode 680/3000 \tAverage Score: -4553.62\n",
      "Epsilon: 0.06551641544060594\n",
      "Average nitrogen amount is 1281.6\n",
      "Average leaching sum is 0.1401838404967333\n",
      "Average Yield is 21647.492553710938\n",
      "Episode 720/3000 \tAverage Score: -2333.76\n",
      "Epsilon: 0.055811496171958616\n",
      "Average nitrogen amount is 1058.8\n",
      "Average leaching sum is 0.13353291723903965\n",
      "Average Yield is 21423.357556152343\n",
      "Episode 760/3000 \tAverage Score: -693.10\n",
      "Epsilon: 0.04754416254314755\n",
      "Average nitrogen amount is 869.2\n",
      "Average leaching sum is 0.12439928120028924\n",
      "Average Yield is 21349.28612060547\n",
      "Episode 800/3000 \tAverage Score: 111.82\n",
      "Epsilon: 0.04050146559348022\n",
      "Average nitrogen amount is 746.0\n",
      "Average leaching sum is 0.12154015936915527\n",
      "Average Yield is 21021.318597412108\n",
      "Episode 840/3000 \tAverage Score: 743.66\n",
      "Epsilon: 0.03450200040291352\n",
      "Average nitrogen amount is 610.4\n",
      "Average leaching sum is 0.118496995065629\n",
      "Average Yield is 20128.08917236328\n",
      "Episode 880/3000 \tAverage Score: 1184.07\n",
      "Epsilon: 0.029391233486480765\n",
      "Average nitrogen amount is 500.8\n",
      "Average leaching sum is 0.1172099873146857\n",
      "Average Yield is 19411.598791503908\n",
      "Episode 920/3000 \tAverage Score: 1303.19\n",
      "Epsilon: 0.02503752233983166\n",
      "Average nitrogen amount is 489.2\n",
      "Average leaching sum is 0.12141078179728935\n",
      "Average Yield is 19990.17462768555\n",
      "Episode 960/3000 \tAverage Score: 1457.24\n",
      "Epsilon: 0.02132872460782966\n",
      "Average nitrogen amount is 463.6\n",
      "Average leaching sum is 0.12632819563177178\n",
      "Average Yield is 20468.11502685547\n",
      "Episode 1000/3000 \tAverage Score: 1288.99\n",
      "Epsilon: 0.01816930953558949\n",
      "Average nitrogen amount is 504.0\n",
      "Average leaching sum is 0.14077395238940368\n",
      "Average Yield is 21119.042407226563\n",
      "Episode 1040/3000 \tAverage Score: 771.52\n",
      "Epsilon: 0.015477897299066653\n",
      "Average nitrogen amount is 613.2\n",
      "Average leaching sum is 0.1610827180466498\n",
      "Average Yield is 21472.529565429686\n",
      "Episode 1080/3000 \tAverage Score: 218.40\n",
      "Epsilon: 0.013185162833579428\n",
      "Average nitrogen amount is 743.2\n",
      "Average leaching sum is 0.1831863418224151\n",
      "Average Yield is 21695.397680664064\n",
      "Episode 1120/3000 \tAverage Score: 532.77\n",
      "Epsilon: 0.011232050167336863\n",
      "Average nitrogen amount is 722.8\n",
      "Average leaching sum is 0.17496182918575356\n",
      "Average Yield is 21697.675146484376\n",
      "Episode 1160/3000 \tAverage Score: 943.71\n",
      "Epsilon: 0.009568251264995805\n",
      "Average nitrogen amount is 649.6\n",
      "Average leaching sum is 0.1618407086751307\n",
      "Average Yield is 21702.87419433594\n",
      "Episode 1200/3000 \tAverage Score: 1493.12\n",
      "Epsilon: 0.00815091019948683\n",
      "Average nitrogen amount is 508.4\n",
      "Average leaching sum is 0.14955062107209813\n",
      "Average Yield is 21705.75778808594\n",
      "Episode 1240/3000 \tAverage Score: 1869.06\n",
      "Epsilon: 0.006943519274326624\n",
      "Average nitrogen amount is 371.6\n",
      "Average leaching sum is 0.13966680688810298\n",
      "Average Yield is 21556.352307128906\n",
      "Episode 1280/3000 \tAverage Score: 1846.63\n",
      "Epsilon: 0.005914978662871382\n",
      "Average nitrogen amount is 253.2\n",
      "Average leaching sum is 0.12946243571144295\n",
      "Average Yield is 19513.666369628907\n",
      "Episode 1320/3000 \tAverage Score: 1569.50\n",
      "Epsilon: 0.005038795342815077\n",
      "Average nitrogen amount is 172.4\n",
      "Average leaching sum is 0.1200421120787854\n",
      "Average Yield is 16213.533972167968\n",
      "Episode 1360/3000 \tAverage Score: 1323.58\n",
      "Epsilon: 0.004292400692186063\n",
      "Average nitrogen amount is 121.2\n",
      "Average leaching sum is 0.11455922370803308\n",
      "Average Yield is 13500.083166503906\n",
      "Episode 1400/3000 \tAverage Score: 1313.69\n",
      "Epsilon: 0.0036565691695638228\n",
      "Average nitrogen amount is 105.2\n",
      "Average leaching sum is 0.11451671077896605\n",
      "Average Yield is 13306.172534179688\n",
      "Episode 1440/3000 \tAverage Score: 1280.25\n",
      "Epsilon: 0.003114923104952546\n",
      "Average nitrogen amount is 104.8\n",
      "Average leaching sum is 0.1152270563415135\n",
      "Average Yield is 12985.386889648438\n",
      "Episode 1480/3000 \tAverage Score: 1244.70\n",
      "Epsilon: 0.002653510845775853\n",
      "Average nitrogen amount is 97.6\n",
      "Average leaching sum is 0.11517270772744666\n",
      "Average Yield is 12604.70701904297\n",
      "Episode 1520/3000 \tAverage Score: 1238.31\n",
      "Epsilon: 0.0022604473919292305\n",
      "Average nitrogen amount is 85.6\n",
      "Average leaching sum is 0.1147709358765627\n",
      "Average Yield is 12496.835778808594\n",
      "Episode 1560/3000 \tAverage Score: 1206.78\n",
      "Epsilon: 0.0019256082634121175\n",
      "Average nitrogen amount is 72.4\n",
      "Average leaching sum is 0.11455572134841929\n",
      "Average Yield is 12143.356372070313\n",
      "Episode 1600/3000 \tAverage Score: 1411.40\n",
      "Epsilon: 0.0016403687152198585\n",
      "Average nitrogen amount is 100.8\n",
      "Average leaching sum is 0.11972800255779874\n",
      "Average Yield is 14226.923474121093\n",
      "Episode 1640/3000 \tAverage Score: 1765.46\n",
      "Epsilon: 0.001397381582224839\n",
      "Average nitrogen amount is 148.8\n",
      "Average leaching sum is 0.1274670638612355\n",
      "Average Yield is 17851.512536621092\n",
      "Episode 1680/3000 \tAverage Score: 2000.13\n",
      "Epsilon: 0.0011903880318026407\n",
      "Average nitrogen amount is 176.0\n",
      "Average leaching sum is 0.13238680547130058\n",
      "Average Yield is 20246.40987548828\n",
      "Episode 1720/3000 \tAverage Score: 1769.45\n",
      "Epsilon: 0.0010140563495926807\n",
      "Average nitrogen amount is 131.6\n",
      "Average leaching sum is 0.1266460691301967\n",
      "Average Yield is 17863.26755371094\n",
      "Episode 1760/3000 \tAverage Score: 1535.03\n",
      "Epsilon: 0.0008638446058568242\n",
      "Average nitrogen amount is 92.8\n",
      "Average leaching sum is 0.12062952161793362\n",
      "Average Yield is 15450.260290527343\n",
      "Episode 1800/3000 \tAverage Score: 1499.03\n",
      "Epsilon: 0.0007358836650129673\n",
      "Average nitrogen amount is 88.8\n",
      "Average leaching sum is 0.12022685309898805\n",
      "Average Yield is 15090.243920898438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1840/3000 \tAverage Score: 1632.48\n",
      "Epsilon: 0.0006268775249175665\n",
      "Average nitrogen amount is 104.8\n",
      "Average leaching sum is 0.12267145548042606\n",
      "Average Yield is 16455.703747558593\n",
      "Episode 1880/3000 \tAverage Score: 1768.69\n",
      "Epsilon: 0.0005340184188486497\n",
      "Average nitrogen amount is 117.6\n",
      "Average leaching sum is 0.12315754875567507\n",
      "Average Yield is 17822.587951660156\n",
      "Episode 1920/3000 \tAverage Score: 1763.40\n",
      "Epsilon: 0.0004549144933966361\n",
      "Average nitrogen amount is 112.8\n",
      "Average leaching sum is 0.11920747065943431\n",
      "Average Yield is 17752.96456298828\n",
      "Episode 1960/3000 \tAverage Score: 1471.94\n",
      "Epsilon: 0.0003875281993989997\n",
      "Average nitrogen amount is 82.8\n",
      "Average leaching sum is 0.11361924091380786\n",
      "Average Yield is 14805.306335449219\n",
      "Episode 2000/3000 \tAverage Score: 1187.77\n",
      "Epsilon: 0.00033012380900006153\n",
      "Average nitrogen amount is 54.4\n",
      "Average leaching sum is 0.1131582358109648\n",
      "Average Yield is 11932.223815917969\n",
      "Episode 2040/3000 \tAverage Score: 1074.75\n",
      "Epsilon: 0.0002812227069868051\n",
      "Average nitrogen amount is 45.2\n",
      "Average leaching sum is 0.11331932151716899\n",
      "Average Yield is 10792.786010742188\n",
      "Episode 2080/3000 \tAverage Score: 1059.55\n",
      "Epsilon: 0.00023956530480045355\n",
      "Average nitrogen amount is 44.8\n",
      "Average leaching sum is 0.11316130566437493\n",
      "Average Yield is 10640.451403808594\n",
      "Episode 2120/3000 \tAverage Score: 1039.01\n",
      "Epsilon: 0.00020407859620961201\n",
      "Average nitrogen amount is 42.4\n",
      "Average leaching sum is 0.11306726075735639\n",
      "Average Yield is 10432.635302734376\n",
      "Episode 2160/3000 \tAverage Score: 1047.82\n",
      "Epsilon: 0.00017384851894800346\n",
      "Average nitrogen amount is 44.4\n",
      "Average leaching sum is 0.11303688731071304\n",
      "Average Yield is 10522.682165527343\n",
      "Episode 2200/3000 \tAverage Score: 1047.61\n",
      "Epsilon: 0.0001480964104112688\n",
      "Average nitrogen amount is 44.0\n",
      "Average leaching sum is 0.11303688731071304\n",
      "Average Yield is 10520.19013671875\n",
      "Episode 2240/3000 \tAverage Score: 1049.95\n",
      "Epsilon: 0.00012615895096157134\n",
      "Average nitrogen amount is 44.8\n",
      "Average leaching sum is 0.11310734049443852\n",
      "Average Yield is 10544.379626464844\n",
      "Episode 2280/3000 \tAverage Score: 1049.21\n",
      "Epsilon: 0.00010747107822211675\n",
      "Average nitrogen amount is 42.4\n",
      "Average leaching sum is 0.11310503000420283\n",
      "Average Yield is 10534.589904785156\n",
      "Episode 2320/3000 \tAverage Score: 1081.31\n",
      "Epsilon: 9.155143227009337e-05\n",
      "Average nitrogen amount is 44.8\n",
      "Average leaching sum is 0.11302302436929892\n",
      "Average Yield is 10857.9642578125\n",
      "Episode 2360/3000 \tAverage Score: 1117.78\n",
      "Epsilon: 7.798995682710675e-05\n",
      "Average nitrogen amount is 48.0\n",
      "Average leaching sum is 0.11335064971846293\n",
      "Average Yield is 11225.88369140625\n",
      "Episode 2400/3000 \tAverage Score: 1147.62\n",
      "Epsilon: 6.643733708009817e-05\n",
      "Average nitrogen amount is 50.4\n",
      "Average leaching sum is 0.11401745286796938\n",
      "Average Yield is 11526.680932617188\n",
      "Episode 2440/3000 \tAverage Score: 1152.17\n",
      "Epsilon: 5.6596002073442036e-05\n",
      "Average nitrogen amount is 50.8\n",
      "Average leaching sum is 0.11410166969549607\n",
      "Average Yield is 11572.649182128906\n",
      "Episode 2480/3000 \tAverage Score: 1088.35\n",
      "Epsilon: 4.8212459912945115e-05\n",
      "Average nitrogen amount is 45.2\n",
      "Average leaching sum is 0.11352832533445906\n",
      "Average Yield is 10928.790173339843\n",
      "Episode 2520/3000 \tAverage Score: 1029.51\n",
      "Epsilon: 4.107076835287798e-05\n",
      "Average nitrogen amount is 40.0\n",
      "Average leaching sum is 0.11300668546956728\n",
      "Average Yield is 10335.215252685546\n",
      "Episode 2560/3000 \tAverage Score: 1029.50\n",
      "Epsilon: 3.498697258222355e-05\n",
      "Average nitrogen amount is 40.0\n",
      "Average leaching sum is 0.113026480246603\n",
      "Average Yield is 10335.077545166016\n",
      "Episode 2600/3000 \tAverage Score: 1034.13\n",
      "Epsilon: 2.9804366939326744e-05\n",
      "Average nitrogen amount is 40.4\n",
      "Average leaching sum is 0.11309763525639425\n",
      "Average Yield is 10381.766528320313\n",
      "Episode 2640/3000 \tAverage Score: 1034.13\n",
      "Epsilon: 2.5389458506774808e-05\n",
      "Average nitrogen amount is 40.4\n",
      "Average leaching sum is 0.11309763525639425\n",
      "Average Yield is 10381.766528320313\n",
      "Episode 2680/3000 \tAverage Score: 895.34\n",
      "Epsilon: 2.1628528617283254e-05\n",
      "Average nitrogen amount is 28.4\n",
      "Average leaching sum is 0.11097339202676665\n",
      "Average Yield is 8981.933795166016\n",
      "Episode 2720/3000 \tAverage Score: 710.19\n",
      "Epsilon: 1.8424703702279314e-05\n",
      "Average nitrogen amount is 12.4\n",
      "Average leaching sum is 0.10812719163511644\n",
      "Average Yield is 7114.37446899414\n",
      "Episode 2760/3000 \tAverage Score: 566.69\n",
      "Epsilon: 1.5695460034461915e-05\n",
      "Average nitrogen amount is 0.0\n",
      "Average leaching sum is 0.10592138633158754\n",
      "Average Yield is 5667.0159912109375\n",
      "Episode 2800/3000 \tAverage Score: 566.69\n",
      "Epsilon: 1.3370498091804627e-05\n",
      "Average nitrogen amount is 0.0\n",
      "Average leaching sum is 0.10592138633158754\n",
      "Average Yield is 5667.0159912109375\n",
      "Episode 2840/3000 \tAverage Score: 566.69\n",
      "Epsilon: 1.1389931791131462e-05\n",
      "Average nitrogen amount is 0.0\n",
      "Average leaching sum is 0.10592138633158754\n",
      "Average Yield is 5667.0159912109375\n",
      "Episode 2880/3000 \tAverage Score: 566.69\n",
      "Epsilon: 9.702745949767184e-06\n",
      "Average nitrogen amount is 0.0\n",
      "Average leaching sum is 0.10592138633158754\n",
      "Average Yield is 5667.0159912109375\n",
      "Episode 2920/3000 \tAverage Score: 566.69\n",
      "Epsilon: 8.265482242749356e-06\n",
      "Average nitrogen amount is 0.0\n",
      "Average leaching sum is 0.10592138633158754\n",
      "Average Yield is 5667.0159912109375\n",
      "Episode 2960/3000 \tAverage Score: 566.69\n",
      "Epsilon: 7.0411198086500665e-06\n",
      "Average nitrogen amount is 0.0\n",
      "Average leaching sum is 0.10592138633158754\n",
      "Average Yield is 5667.0159912109375\n",
      "Episode 3000/3000 \tAverage Score: 566.69\n",
      "Epsilon: 5.9981216707899415e-06\n",
      "Average nitrogen amount is 0.0\n",
      "Average leaching sum is 0.10592138633158754\n",
      "Average Yield is 5667.0159912109375\n",
      "Time Elapse: 1890.78 seconds\n"
     ]
    }
   ],
   "source": [
    "scores, list_eps, action_amount_list, leaching_sum_list = dqn(n_episodes=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c689d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42bc092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b2d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1de58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0df360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f9f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03fd2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14023e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46222b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95dd00b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a302c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebf175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ad242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6db0e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e90c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17375c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e7c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37677d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa1df64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae5626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6035404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aeab62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a3a4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388e717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1cd996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64630fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88ee21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0cd031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307728e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb63b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e0f2f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#print(len(scores))\n",
    "#action_list=action_list/100\n",
    "a = pd.DataFrame(scores)\n",
    "a.to_csv('1_IOWA_money.csv', index=False, header=False)\n",
    "#b = pd.DataFrame(action_list)\n",
    "#b.to_csv('Action with constant k2=0.5.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894bc9c8",
   "metadata": {},
   "source": [
    "### Given a constant input: reward = 19847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a1562cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(scores[-300:])\n",
    "plt.title(\"Nitrogen Management\")\n",
    "plt.xlabel(\"Traing Steps\")\n",
    "plt.ylabel(\"Cumulative Rewards\")\n",
    "plt.savefig(\"Cumulative reward with linear penalty last 300.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f466aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)\n",
    "plt.yscale('symlog')\n",
    "plt.title(\"Nitrogen Management\")\n",
    "plt.xlabel(\"Traing Steps\")\n",
    "plt.ylabel(\"Cumulative Rewards\")\n",
    "plt.savefig(\"Cumulative reward (log scale) linear penalty.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a057e5d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action_number_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43maction_number_list\u001b[49m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(action_number_list)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of actions of episode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'action_number_list' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "print(action_number_list)\n",
    "plt.plot(action_number_list)\n",
    "plt.title(\"Number of actions of episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Number of actions\")\n",
    "plt.savefig(\"Number of actions with linear penalty.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "print(action_list)\n",
    "plt.plot(action_list)\n",
    "plt.title(\"Naction of last episode\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Nitrogen input\")\n",
    "plt.savefig(\"Action with linear penalty.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52272ab2",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
